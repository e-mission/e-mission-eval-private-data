{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env DB_HOST=mongodb://localhost/openpath_prod_ca_ebike\n",
    "import emission.core.get_database as edb\n",
    "import emission.storage.timeseries.aggregate_timeseries as esta\n",
    "import emission.storage.timeseries.builtin_timeseries as estb\n",
    "import emission.core.get_database as gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of documents with metadata.key = \"stats/pipeline_time\" in Stage_timeseries\n",
    "pipeline_time_count = gdb.get_timeseries_db().count_documents({\n",
    "    \"metadata.key\": \"stats/pipeline_time\"\n",
    "})\n",
    "print(f\"Total documents in Stage_timeseries with metadata.key 'stats/pipeline_time': {pipeline_time_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of documents with metadata.key = \"stats/dashboard_time\" in Stage_timeseries\n",
    "dashboard_time_count = gdb.get_timeseries_db().count_documents({\n",
    "    \"metadata.key\": \"stats/dashboard_time\"\n",
    "})\n",
    "print(f\"Total documents in Stage_timeseries with metadata.key 'stats/dashboard_time': {dashboard_time_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch documents with metadata.key = \"stats/pipeline_time\"\n",
    "pipeline_docs_cursor = gdb.get_timeseries_db().find({\n",
    "    \"metadata.key\": \"stats/pipeline_time\"\n",
    "})\n",
    "\n",
    "# Display a sample of the documents\n",
    "import pprint\n",
    "pipeline_docs = list(pipeline_docs_cursor)\n",
    "if pipeline_docs:\n",
    "    single_doc = pipeline_docs[0]\n",
    "    print(\"Single Document:\")\n",
    "    pprint.pprint(single_doc)\n",
    "else:\n",
    "    print(\"No documents found for 'stats/pipeline_time'.\")\n",
    "\n",
    "# Fetch multiple documents\n",
    "pipeline_docs_sample = pipeline_docs[:5]  # Get first 5 documents\n",
    "print(\"\\nMultiple Documents:\")\n",
    "for doc in pipeline_docs_sample:\n",
    "    pprint.pprint(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "documents_cursor = gdb.get_timeseries_db().find({\"metadata.key\": \"stats/pipeline_time\"})\n",
    "documents = list(documents_cursor)\n",
    "df = pd.json_normalize(documents)\n",
    "\n",
    "# Display DataFrame columns and data types\n",
    "print(\"\\nDataFrame Columns and Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nDataFrame Description:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst Five Rows of DataFrame:\")\n",
    "print(df.head())\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the aggregation pipeline as a function\n",
    "def process_pipeline_data(data_df):\n",
    "    # Filter documents where metadata.key is 'stats/pipeline_time'\n",
    "    filtered_df = data_df[data_df['metadata.key'] == 'stats/pipeline_time']\n",
    "    \n",
    "    # Group by year, month, and day to calculate statistics on `data.reading`\n",
    "    grouped_df = filtered_df.groupby(\n",
    "        ['metadata.write_local_dt.year', 'metadata.write_local_dt.month', 'metadata.write_local_dt.day']\n",
    "    ).agg(\n",
    "        avgPipelineTime=('data.reading', 'mean'),\n",
    "        minPipelineTime=('data.reading', 'min'),\n",
    "        maxPipelineTime=('data.reading', 'max'),\n",
    "        count=('data.reading', 'size'),\n",
    "        medianPipelineTime=('data.reading', 'median'),\n",
    "        stdPipelineTime=('data.reading', 'std')\n",
    "    ).reset_index()\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "# Convert the aggregation results into a Pandas DataFrame for easier data manipulation\n",
    "agg_df = process_pipeline_data(df)\n",
    "\n",
    "# Display the first few rows of the aggregated DataFrame\n",
    "print(\"\\nAggregation Results:\")\n",
    "print(agg_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the visual style\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the distribution of pipeline times\n",
    "def visualize_pipeline_time_distribution(data_df):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Box Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x='data.name', y='data.reading', data=data_df)\n",
    "    plt.title('Box Plot of Pipeline Times by Stage')\n",
    "    plt.xlabel('Stage')\n",
    "    plt.ylabel('Pipeline Time (ms)')  # Added units in milliseconds\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Violin Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.violinplot(x='data.name', y='data.reading', data=data_df, inner='quartile')\n",
    "    plt.title('Violin Plot of Pipeline Times by Stage')\n",
    "    plt.xlabel('Stage')\n",
    "    plt.ylabel('Pipeline Time (ms)')  # Added units in milliseconds\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the distribution of pipeline times\n",
    "visualize_pipeline_time_distribution(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Analysis: Histogram and KDE\n",
    "def plot_histogram_kde(data_df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data_df['data.reading'], bins=30, kde=True)\n",
    "    plt.title('Histogram and KDE of Pipeline Times')\n",
    "    plt.xlabel('Pipeline Time (ms)')  # Units in milliseconds for x-axis\n",
    "    plt.ylabel('Frequency')  # No units needed for frequency on y-axis\n",
    "    plt.show()\n",
    "\n",
    "# Plot the histogram and KDE of pipeline times\n",
    "plot_histogram_kde(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot: Pipeline Time vs. Timestamp\n",
    "def plot_scatter_time(data_df):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Convert write_fmt_time to datetime if not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(data_df['metadata.write_fmt_time']):\n",
    "        data_df['metadata.write_fmt_time'] = pd.to_datetime(data_df['metadata.write_fmt_time'], errors='coerce')\n",
    "    \n",
    "    # Ensure there are no missing values in 'metadata.write_fmt_time' and 'data.reading'\n",
    "    data_df = data_df.dropna(subset=['metadata.write_fmt_time', 'data.reading', 'data.name'])\n",
    "    \n",
    "    # Create the scatter plot with the data parameter\n",
    "    sns.scatterplot(\n",
    "        x='metadata.write_fmt_time',\n",
    "        y='data.reading',\n",
    "        hue='data.name',\n",
    "        data=data_df,           # Pass the DataFrame here\n",
    "        alpha=0.6,\n",
    "        palette='tab10',\n",
    "        s=50\n",
    "    )\n",
    "    \n",
    "    plt.title('Pipeline Time Over Time by Stage')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Pipeline Time (ms)')\n",
    "    plt.legend(title='Stage', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Execute the corrected function\n",
    "plot_scatter_time(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "def correlation_analysis(data_df):\n",
    "    numeric_cols = ['data.reading', 'metadata.write_local_dt.year', 'metadata.write_local_dt.month', 'metadata.write_local_dt.day']\n",
    "    corr = data_df[numeric_cols].corr()\n",
    "    print(\"\\nCorrelation Matrix:\")\n",
    "    print(corr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()\n",
    "    print(\"Correlation Heatmap displayed.\")\n",
    "\n",
    "correlation_analysis(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics by Stage\n",
    "def summary_stats_by_stage(data_df):\n",
    "    summary = data_df.groupby('data.name')['data.reading'].agg(['mean', 'median', 'min', 'max', 'std', 'count']).reset_index()\n",
    "    print(\"\\nSummary Statistics by Stage:\")\n",
    "    print(summary)\n",
    "    \n",
    "    # Plotting the summary statistics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='data.name', y='mean', data=summary, palette='viridis')\n",
    "    plt.title('Average Pipeline Time by Stage')\n",
    "    plt.xlabel('Stage')\n",
    "    plt.ylabel('Average Pipeline Time')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "summary_stats_by_stage(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Outliers\n",
    "def identify_outliers(data_df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='data.name', y='data.reading', data=data_df)\n",
    "    plt.title('Outliers in Pipeline Times by Stage')\n",
    "    plt.xlabel('Stage')\n",
    "    plt.ylabel('Pipeline Time')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "identify_outliers(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get pipeline time by stage for each user and across all users\n",
    "def get_pipeline_time_by_stage(data_df):\n",
    "    # Calculate the average time for each stage (data.name) for each user\n",
    "    user_stage_avg = data_df.groupby(['user_id', 'data.name'])['data.reading'].mean().reset_index()\n",
    "    \n",
    "    # Calculate the average time for each stage across all users\n",
    "    all_users_stage_avg = data_df.groupby('data.name')['data.reading'].mean().reset_index()\n",
    "    \n",
    "    return user_stage_avg, all_users_stage_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pipeline time by stage\n",
    "user_stage_avg_df, all_users_stage_avg_df = get_pipeline_time_by_stage(df)\n",
    "\n",
    "#Display the results\n",
    "print(\"Average Stage Times for Each User (First 5 Rows):\")\n",
    "print(user_stage_avg_df)\n",
    "\n",
    "print(\"\\nAverage Stage Times Across All Users:\")\n",
    "print(all_users_stage_avg_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Average Pipeline Time Across All Users by Stage\n",
    "def plot_average_pipeline_time(all_users_stage_avg_df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='data.name', y='data.reading', data=all_users_stage_avg_df, palette='magma')\n",
    "    plt.title('Average Pipeline Time Across All Users by Stage')\n",
    "    plt.xlabel('Stage')\n",
    "    plt.ylabel('Average Pipeline Time (ms)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "plot_average_pipeline_time(all_users_stage_avg_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize User-Specific Pipeline Times\n",
    "def plot_user_pipeline_times(user_stage_avg_df):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='data.name', y='data.reading', data=user_stage_avg_df)\n",
    "    plt.title('Pipeline Time Distribution by Stage Across Users')\n",
    "    plt.xlabel('Stage')\n",
    "    plt.ylabel('Pipeline Time (ms)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "plot_user_pipeline_times(user_stage_avg_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate Low Average Pipeline Time\n",
    "def investigate_low_avg(data_df):\n",
    "    # Check for zeros or extremely low values\n",
    "    low_threshold = data_df['data.reading'].quantile(0.05)\n",
    "    low_values = data_df[data_df['data.reading'] < low_threshold]\n",
    "    print(f\"\\nNumber of records with pipeline time below the 5th percentile ({low_threshold}): {low_values.shape[0]}\")\n",
    "    \n",
    "    if not low_values.empty:\n",
    "        print(\"\\nSample of Low Pipeline Time Records:\")\n",
    "        print(low_values.head())\n",
    "\n",
    "investigate_low_avg(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
